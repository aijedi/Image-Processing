{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import face_recognition\n",
    "\n",
    "# import the necessary packages\n",
    "from imutils.face_utils import FaceAligner\n",
    "from imutils.face_utils import rect_to_bb\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Alignment of Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Ritu Projects\\\\Project 5 - BMI Regression'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor and the face aligner\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"req_models\\shape_predictor_68_face_landmarks.dat\")\n",
    "fa = FaceAligner(predictor, desiredFaceWidth=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_image(path_to_image,save_path):\n",
    "\n",
    "    # load the input image, resize it, and convert it to grayscale\n",
    "    image = cv2.imread(path_to_image)\n",
    "    image = imutils.resize(image, width=800)\n",
    "    \n",
    "    try:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    except:\n",
    "        gray = cv2.imread(path_to_image,0)\n",
    "    rects = detector(gray, 2)\n",
    "    \n",
    "    i = 0\n",
    "    # loop over the face detections\n",
    "    for rect in rects:\n",
    "        # extract the ROI of the *original* face, then align the face\n",
    "        # using facial landmarks\n",
    "        (x, y, w, h) = rect_to_bb(rect)\n",
    "        \n",
    "        faceAligned = fa.align(image, gray, rect)\n",
    "        \n",
    "        \n",
    "        print (faceAligned.shape)\n",
    "        name = save_path.split('.')\n",
    "        if (i == 0):\n",
    "            cv2.imwrite(save_path ,faceAligned)\n",
    "            cv2.waitKey(0)\n",
    "            \n",
    "        else:\n",
    "            cv2.imwrite(name[0] + str(i) + '.' + name[1] ,faceAligned)\n",
    "            cv2.waitKey(0)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Face Maskout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_position(sample_img,r_start,r_stop,r_index,c_start,c_stop,c_index,flag):\n",
    "    for i in range(r_start,r_stop,r_index):\n",
    "        for j in range(c_start,c_stop,c_index):\n",
    "            x,y=i,j\n",
    "            if(flag!=1):\n",
    "                temp=y\n",
    "                y=x\n",
    "                x=temp\n",
    "            if(not np.array_equal(sample_img[x][y][:],np.array([128,128,128]))):\n",
    "                sample_img[x][y][:]=[0,0,0]\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    return sample_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_face(sample_img):\n",
    "    row=len(sample_img)\n",
    "    column=len(sample_img[0])\n",
    "    sample_img=align_position(sample_img,0,row,1,0,column,1,1)\n",
    "    sample_img=align_position(sample_img,0,row,1,column-1,-1,-1,1)\n",
    "    sample_img=align_position(sample_img,0,column,1,0,row,1,-1)\n",
    "    sample_img=align_position(sample_img,0,column,1,row-1,-1,-1,-1)\n",
    "    return sample_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_points(points1,points2,point_list):\n",
    "    y1,x1 = points1\n",
    "    y2,x2 = points2\n",
    "    try:\n",
    "        m=(y2-y1)/(x2-x1)\n",
    "    except:\n",
    "        m=(y2-y1)/1\n",
    "    n=y1-m*x1\n",
    "    point_list.append((x2,y2))\n",
    "    if(abs(y2-y1)>abs(x2-x1)):\n",
    "        for y in range(y1, y2, 1 if y2>y1 else -1):\n",
    "            try:\n",
    "                x=round((y-n)/m)\n",
    "            except:\n",
    "                x=n\n",
    "            point_list.append((x,y))\n",
    "    else:\n",
    "        for x in range(x1,x2, 1 if x2>x1 else -1):\n",
    "            y=round(m*x+n)\n",
    "            point_list.append((x,y))\n",
    "            \n",
    "    return point_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bound(point_list,i,j):\n",
    "    lst=[]\n",
    "    for (x,y) in point_list:\n",
    "        if(x>=i):\n",
    "            a=i-2\n",
    "            b=y\n",
    "        if(y>=j):\n",
    "            b=j-2\n",
    "            a=x\n",
    "        if(x<i and y<j):\n",
    "            a,b=x,y\n",
    "        lst.append((a,b))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_positions(image_name,save_dir):\n",
    "    image = face_recognition.load_image_file(image_name)\n",
    "    face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "    \n",
    "    sample_img=np.array(cv2.imread(image_name))\n",
    "    point_list=[]\n",
    "    #print face_landmarks_list\n",
    "    for data in face_landmarks_list[0]:\n",
    "        if data in ['chin','left_eyebrow','right_eyebrow']:\n",
    "            points=face_landmarks_list[0][data]\n",
    "            points=remove_bound(points,len(sample_img[1]),len(sample_img[0]))\n",
    "            for ln in range(len(points)-1):\n",
    "                point_list=connect_points(points[ln],points[ln+1],point_list)\n",
    "    \n",
    "    point_list=connect_points(face_landmarks_list[0]['chin'][0],face_landmarks_list[0]['left_eyebrow'][0],point_list)\n",
    "    point_list=connect_points(face_landmarks_list[0]['chin'][16],face_landmarks_list[0]['right_eyebrow'][4],point_list)\n",
    "    point_list=connect_points(face_landmarks_list[0]['left_eyebrow'][4],face_landmarks_list[0]['right_eyebrow'][0],point_list)\n",
    "\n",
    "    \n",
    "    point_list=remove_bound(point_list,len(sample_img[1]),len(sample_img[0]))\n",
    "\n",
    "    #print (point_list)\n",
    "    for (x,y) in point_list:\n",
    "        sample_img[int(x)][int(y)][:]=[128,128,128]\n",
    "        \n",
    "    sample_img=produce_face(sample_img)\n",
    "    \n",
    "    cv2.imwrite(save_dir,sample_img)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Enhancement of Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageFilter, ImageEnhance \n",
    "import numpy as np \n",
    "import cv2 \n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_enhance(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    enhancer = ImageEnhance.Sharpness(image)\n",
    "    enhancer= ImageEnhance.Contrast(image) \n",
    "    enhanced_im = enhancer.enhance(1.8)\n",
    "    enhanced_im.save(image_path)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharpening of Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpen_image(image_path):\n",
    "    image = cv2.imread(image_path,cv2.COLOR_BGR2RGB)\n",
    "    image= cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    kernel_sharpening = np.array([[-1,-1,-1], \n",
    "                              [-1, 9,-1],\n",
    "                              [-1,-1,-1]])\n",
    "    sharpened = cv2.filter2D(image, -1, kernel_sharpening)\n",
    "    \n",
    "    print(image_path)\n",
    "    plt.imsave(image_path , sharpened)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to perform Preprocessing the image:Â¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image_path):\n",
    "    align_image(image_path,image_path)\n",
    "    face_positions(image_path,image_path)\n",
    "    #sharpen_images(image_path)\n",
    "    image_enhance(image_path)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
